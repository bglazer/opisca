{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07713a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric as pyg\n",
    "import torch\n",
    "import numpy as np\n",
    "import scanpy\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a214884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f70f8d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e248f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7efe293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66274089",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = !find 'output/datasets/predict_modality/' -type f -name '*train*h5ad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9620eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for filename in data_files:\n",
    "    data[filename.split('/')[-1]] = scanpy.read_h5ad(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "265f3723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proteins_to_idxs(data):\n",
    "    indexes = []\n",
    "    proteins = data.var.index.to_list()\n",
    "    for protein_name in proteins:\n",
    "        protein_name = protein_name.upper()\n",
    "        if protein_name in node_idxs['gene_name']:\n",
    "            indexes.append(node_idxs['gene_name'][protein_name])\n",
    "        else:\n",
    "            indexes.append(None)\n",
    "    return indexes, data.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22034c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genes_to_idxs(data):\n",
    "    indexes = []\n",
    "    genes = data.var['gene_ids'].to_list()\n",
    "    for gene_id in genes:\n",
    "        if gene_id in node_idxs['gene']:\n",
    "            indexes.append(node_idxs['gene'][gene_id])\n",
    "        else:\n",
    "            indexes.append(None)\n",
    "    return indexes, data.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b0f7f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atac_to_idxs(data):\n",
    "    indexes = {}\n",
    "    regions = data.var.index.to_list()\n",
    "    for region in regions:\n",
    "        if region in node_idxs['atac_region']:\n",
    "            indexes.append(node_idxs['atac_region'][region])\n",
    "        else:\n",
    "            indexes.append(None)\n",
    "    return indexes, data.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "418c6aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "from torch import tensor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9440f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfbc860",
   "metadata": {},
   "source": [
    "## Protein to gene expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9829048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = torch.load('input/graph_with_embeddings.torch')\n",
    "node_idxs = pickle.load(open('input/nodes_by_type.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fc4b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_data = data['openproblems_bmmc_cite_phase1_mod2.censor_dataset.output_train_mod1.h5ad']\n",
    "protein_idxs, protein_expression = proteins_to_idxs(protein_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd90370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_data =    data['openproblems_bmmc_cite_phase1_mod2.censor_dataset.output_train_mod2.h5ad']\n",
    "gene_idxs, gene_expression = genes_to_idxs(gene_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffbddbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d98411ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = pyg.transforms.ToUndirected()(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "882cbfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "890059d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_mask = torch.zeros((len(node_idxs['gene_name']),1), dtype=bool, device=device)\n",
    "protein_mask[protein_idxs] = 1\n",
    "graph['gene_name']['mask'] = protein_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae87f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_mask = torch.zeros((len(node_idxs['gene']),1), dtype=bool, device=device)\n",
    "gene_mask[[idx for idx in gene_idxs if idx]] = 1\n",
    "graph['gene']['mask'] = gene_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b1b79cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12437, device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph['gene'].mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16060f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(134, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph['gene_name'].mask.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53c7692",
   "metadata": {},
   "source": [
    "## Function to create new data object with expression values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d52071a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_expression(graph, cell_idx):\n",
    "    newgraph = HeteroData()\n",
    "    \n",
    "    gene = tensor(gene_expression[cell_idx].todense())\n",
    "    protein = tensor(protein_expression[cell_idx].todense())\n",
    "        \n",
    "    expression = dict()\n",
    "\n",
    "    for node_type in ['gene_name', 'gene', 'atac_region']:\n",
    "        expression[node_type] = torch.ones((\n",
    "            len(node_idxs[node_type]),\n",
    "            1\n",
    "        ),device=device)*-1\n",
    "    \n",
    "    for i in range(gene.shape[1]):\n",
    "        if gene_idxs[i]:\n",
    "            expression['gene'][gene_idxs[i]] = gene[:,i]\n",
    "    \n",
    "    for i in range(protein.shape[1]):\n",
    "        if protein_idxs[i]:\n",
    "            expression['gene_name'][protein_idxs[i]] = protein[:,i]\n",
    "\n",
    "    newgraph['gene_name'].y = expression['gene_name']\n",
    "    newgraph['gene_name'].x = torch.ones((len(node_idxs['gene_name']),1),device=device)*-1\n",
    "\n",
    "    newgraph['gene'].x = torch.cat([\n",
    "        graph['gene'].x,\n",
    "        expression['gene']\n",
    "    ], dim=1)\n",
    "        \n",
    "    newgraph['atac_region'].x = torch.cat([\n",
    "        graph['atac_region'].x,\n",
    "        expression['atac_region']\n",
    "    ], dim=1)\n",
    "    \n",
    "    newgraph['tad'].x = graph['tad'].x\n",
    "    newgraph['protein'].x = graph['protein'].x\n",
    "    \n",
    "    for edge_type, store in graph._edge_store_dict.items():\n",
    "        for k,v in store.items():\n",
    "            newgraph[edge_type][k]=v\n",
    "    \n",
    "    return newgraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb39d3f",
   "metadata": {},
   "source": [
    "## EARL = Expression and Representation Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e8ec8",
   "metadata": {},
   "source": [
    "✓ For each cell, create a data vector.\n",
    "\n",
    "✓ Data level batching\n",
    "\n",
    "Graph level batching (is this necessary?)\n",
    "\n",
    "✓ Metapath or TransE for featureless (all) nodes?\n",
    "\n",
    "Random masking (self supervision)\n",
    "\n",
    "✓ Backprop loss of just unknown\n",
    "\n",
    "✓ Make most graphs undirected. Remove incoming edges to known nodes?\n",
    "\n",
    "✓ Create a GNN\n",
    "\n",
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c142150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import to_hetero, SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b80bc438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv, GCNConv, SAGEConv, GATConv, Linear\n",
    "\n",
    "class EaRL(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.linear = Linear(hidden_channels,1)\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                ('tad', 'overlaps', 'atac_region'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('tad', 'overlaps', 'gene'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('atac_region', 'rev_overlaps', 'tad'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('atac_region', 'overlaps', 'gene'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('protein', 'coexpressed', 'protein'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('protein', 'tf_interacts', 'gene'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('protein', 'trrust_interacts', 'gene'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('gene', 'rev_overlaps', 'tad'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('gene', 'rev_overlaps', 'atac_region'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('gene', 'rev_trrust_interacts', 'protein'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('gene', 'rev_tf_interacts', 'protein'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('protein', 'rev_associated', 'gene'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('gene', 'associated', 'protein'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('protein', 'is_named', 'gene_name'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('gene_name', 'rev_is_named', 'protein'): SAGEConv((-1, -1), hidden_channels)\n",
    "            })\n",
    "\n",
    "            self.convs.append(conv)\n",
    "        self.name_conv = HeteroConv({('protein', 'is_named', 'gene_name'): SAGEConv((-1, -1), 1)})\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        #         gene_names = x_dict['gene_name']\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "            x_dict['gene_name'] = self.linear(x_dict['gene_name'])\n",
    "            #         x_dict['gene_name'] = gene_names\n",
    "            #         names = self.name_conv(x_dict, edge_index_dict)\n",
    "            #         x_dict['gene_name'] = names['gene_name']\n",
    "\n",
    "            return x_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407ce410",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d0e45e",
   "metadata": {},
   "source": [
    "## TODO trim the gene names, we have way more gene names than we have in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a14245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cells = gene_expression.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2b7e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_size = int(num_cells*.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb62dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set_size = num_cells - train_set_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "520e5e3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Loss: 2065.065185546875\n",
      "Batch: 1, Loss: 2044.35107421875\n",
      "Batch: 2, Loss: 1143.814208984375\n",
      "Batch: 3, Loss: 1375.7183837890625\n",
      "Batch: 4, Loss: 1414.009033203125\n",
      "Batch: 5, Loss: 780.3236694335938\n",
      "Batch: 6, Loss: 626.9471435546875\n",
      "Batch: 7, Loss: 681.8627319335938\n",
      "Batch: 8, Loss: 2575.94775390625\n",
      "Batch: 9, Loss: 621.9063720703125\n",
      "Batch: 10, Loss: 1047.905517578125\n",
      "Batch: 11, Loss: 815.927001953125\n",
      "Batch: 12, Loss: 1354.922607421875\n",
      "Batch: 13, Loss: 681.4703369140625\n",
      "Batch: 14, Loss: 1164.22314453125\n",
      "Batch: 15, Loss: 879.9378051757812\n",
      "Batch: 16, Loss: 685.1246337890625\n",
      "Batch: 17, Loss: 624.9552001953125\n",
      "Batch: 18, Loss: 1130.24853515625\n",
      "Batch: 19, Loss: 1068.029296875\n",
      "Batch: 20, Loss: 1232.4345703125\n",
      "Batch: 21, Loss: 2133.94970703125\n",
      "Batch: 22, Loss: 1174.459228515625\n",
      "Batch: 23, Loss: 1549.676513671875\n",
      "Batch: 24, Loss: 549.6134033203125\n",
      "Batch: 25, Loss: 1190.075927734375\n",
      "Batch: 26, Loss: 914.02001953125\n",
      "Batch: 27, Loss: 945.0771484375\n",
      "Batch: 28, Loss: 1129.7239990234375\n",
      "Batch: 29, Loss: 599.9124755859375\n",
      "Batch: 30, Loss: 1319.4400634765625\n",
      "Batch: 31, Loss: 751.1270141601562\n",
      "Batch: 32, Loss: 1720.869873046875\n",
      "Batch: 33, Loss: 1601.323486328125\n",
      "Batch: 34, Loss: 1395.76904296875\n",
      "Batch: 35, Loss: 623.24365234375\n",
      "Batch: 36, Loss: 942.3756103515625\n",
      "Batch: 37, Loss: 824.483642578125\n",
      "Batch: 38, Loss: 561.8115234375\n",
      "Batch: 39, Loss: 551.8211669921875\n",
      "Batch: 40, Loss: 2043.0150146484375\n",
      "Batch: 41, Loss: 1122.274169921875\n",
      "Batch: 42, Loss: 3373.789794921875\n",
      "Batch: 43, Loss: 1082.705322265625\n",
      "Batch: 44, Loss: 1117.668212890625\n",
      "Batch: 45, Loss: 537.915283203125\n",
      "Batch: 46, Loss: 641.45361328125\n",
      "Batch: 47, Loss: 964.4632568359375\n",
      "Batch: 48, Loss: 872.8880615234375\n",
      "Batch: 49, Loss: 3338.705810546875\n",
      "Batch: 50, Loss: 1479.611328125\n",
      "Batch: 51, Loss: 638.5607299804688\n",
      "Batch: 52, Loss: 973.6694946289062\n",
      "Batch: 53, Loss: 651.8583984375\n",
      "Batch: 54, Loss: 805.4011840820312\n",
      "Batch: 55, Loss: 1236.111572265625\n",
      "Batch: 56, Loss: 1164.50341796875\n",
      "Batch: 57, Loss: 2891.707763671875\n",
      "Batch: 58, Loss: 771.8155517578125\n",
      "Batch: 59, Loss: 882.7640991210938\n",
      "Batch: 60, Loss: 2696.061767578125\n",
      "Batch: 61, Loss: 1164.533935546875\n",
      "Batch: 62, Loss: 1028.575439453125\n",
      "Batch: 63, Loss: 1135.09423828125\n",
      "Batch: 64, Loss: 608.9613647460938\n",
      "Batch: 65, Loss: 1256.02978515625\n",
      "Batch: 66, Loss: 912.19287109375\n",
      "Batch: 67, Loss: 541.3779296875\n",
      "Batch: 68, Loss: 489.6367492675781\n",
      "Batch: 69, Loss: 728.8568725585938\n",
      "Batch: 70, Loss: 533.5130004882812\n",
      "Batch: 71, Loss: 1085.9520263671875\n",
      "Batch: 72, Loss: 2184.614501953125\n",
      "Batch: 73, Loss: 2715.939453125\n",
      "Batch: 74, Loss: 978.3272705078125\n",
      "Batch: 75, Loss: 609.417724609375\n",
      "Batch: 76, Loss: 481.79412841796875\n",
      "Batch: 77, Loss: 565.52197265625\n",
      "Batch: 78, Loss: 579.8675537109375\n",
      "Batch: 79, Loss: 1038.6129150390625\n",
      "Batch: 80, Loss: 566.4237060546875\n",
      "Batch: 81, Loss: 1044.517822265625\n",
      "Batch: 82, Loss: 2529.541015625\n",
      "Batch: 83, Loss: 597.9986572265625\n",
      "Batch: 84, Loss: 705.63818359375\n",
      "Batch: 85, Loss: 582.60400390625\n",
      "Batch: 86, Loss: 1231.025634765625\n",
      "Batch: 87, Loss: 3051.30224609375\n",
      "Batch: 88, Loss: 878.9520263671875\n",
      "Batch: 89, Loss: 1300.514404296875\n",
      "Batch: 90, Loss: 955.748779296875\n",
      "Batch: 91, Loss: 1786.8787841796875\n",
      "Batch: 92, Loss: 715.744384765625\n",
      "Batch: 93, Loss: 1037.7696533203125\n",
      "Batch: 94, Loss: 2747.066650390625\n",
      "Batch: 95, Loss: 934.9088134765625\n",
      "Batch: 96, Loss: 557.2459716796875\n",
      "Batch: 97, Loss: 599.8878784179688\n",
      "Batch: 98, Loss: 912.0553588867188\n",
      "Batch: 99, Loss: 813.7015991210938\n",
      "Batch: 100, Loss: 626.514404296875\n",
      "Batch: 101, Loss: 1422.9246826171875\n",
      "Batch: 102, Loss: 2503.032470703125\n",
      "Batch: 103, Loss: 531.977294921875\n",
      "Batch: 104, Loss: 616.0236206054688\n",
      "Batch: 105, Loss: 587.5702514648438\n",
      "Batch: 106, Loss: 613.3895263671875\n",
      "Batch: 107, Loss: 981.0025634765625\n",
      "Batch: 108, Loss: 948.3810424804688\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_987143/3157532175.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mnewgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mappend_expression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mearl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gene_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gene_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/opisca/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_987143/332485087.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_dict, edge_index_dict)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#         gene_names = x_dict['gene_name']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mx_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mx_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mx_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gene_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gene_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/opisca/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/opisca/lib/python3.8/site-packages/torch_geometric/nn/conv/hetero_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_dict, edge_index_dict, **kwargs_dict)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 out = conv(x=(x_dict[src], x_dict[dst]), edge_index=edge_index,\n\u001b[0m\u001b[1;32m     95\u001b[0m                            **kwargs)\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/opisca/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/opisca/lib/python3.8/site-packages/torch_geometric/nn/conv/sage_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# propagate_type: (x: OptPairTensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_l\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/opisca/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                     \u001b[0maggr_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0maggr_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maggr_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/opisca/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msegment_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             return scatter(inputs, index, dim=self.node_dim, dim_size=dim_size,\n\u001b[0m\u001b[1;32m    340\u001b[0m                            reduce=self.aggr)\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/opisca/lib/python3.8/site-packages/torch_scatter/scatter.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, out, dim_size, reduce)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscatter_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscatter_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'min'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscatter_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/opisca/lib/python3.8/site-packages/torch_scatter/scatter.py\u001b[0m in \u001b[0;36mscatter_mean\u001b[0;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# earl = to_hetero(EaRL(64), graph.metadata(), aggr='sum')\n",
    "earl = EaRL(hidden_channels=64, num_layers=3)\n",
    "earl = earl.to('cuda:0')\n",
    "optimizer = torch.optim.Adam(params=earl.parameters(), lr=.001)\n",
    "earl.train()\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "cell_idxs = list(range(gene_expression.shape[0]))\n",
    "random.shuffle(cell_idxs)\n",
    "cell_idxs = cell_idxs[:train_set_size]\n",
    "\n",
    "batch_size = 20\n",
    "for epoch in range(n_epochs):\n",
    "    mask = graph['gene_name']['mask']\n",
    "    total_loss = 0\n",
    "    batch_start = 0\n",
    "    batch_end = batch_size\n",
    "    \n",
    "    num_predictions = min(batch_size-batch_end+len(cell_idxs), batch_size)\n",
    "    \n",
    "    batch_idx = 0\n",
    "    while batch_end < len(cell_idxs)+batch_size:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = torch.zeros((\n",
    "            num_predictions,\n",
    "            mask.sum()\n",
    "        ), device=device)\n",
    "        for i,idx in enumerate(cell_idxs[batch_start:batch_end]):\n",
    "            newgraph = append_expression(graph, idx)\n",
    "            predictions[i] = earl(newgraph.x_dict, newgraph.edge_index_dict)['gene_name'][mask].flatten()\n",
    "\n",
    "        y = newgraph['gene_name'].y[mask]\n",
    "\n",
    "        loss = ((predictions - y)**2).sum()\n",
    "        print(f'Batch: {batch_idx}, Loss: {float(loss)}')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_start += batch_size\n",
    "        batch_end += batch_size\n",
    "        batch_idx += 1\n",
    "        \n",
    "    print({'Epoch: {epoch}, Epoch loss: {float(loss)}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ca7c6",
   "metadata": {},
   "source": [
    "## Maybe try sampling zero/one for gene dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4110f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e261d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli = Bernoulli(torch.tensor([.5,.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f3e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli.sample((10,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opisca]",
   "language": "python",
   "name": "conda-env-opisca-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
